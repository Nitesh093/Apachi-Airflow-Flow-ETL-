# ğŸš€ Apache Airflow ETL â€“ S3 â†’ PostgreSQL

A complete ETL (Extractâ€“Transformâ€“Load) pipeline built using **Apache Airflow**, where data is extracted from **AWS S3**, transformed using Python scripts, and loaded into **PostgreSQL**.  
The project runs locally using **Docker Compose** and supports environment-based configuration.

---

## âœ¨ Features

- ğŸ“¥ Extract raw data from AWS S3  
- ğŸ”„ Transform & clean CSV/JSON files  
- ğŸ—ƒ Load processed records into PostgreSQL  
- â± Automated scheduling with Apache Airflow DAG  
- ğŸ³ Containerized using Docker Compose  
- ğŸ” Secure configuration through environment variables  

---

## ğŸ“ Project Structure

Apachi-Airflow-Flow-ETL/
â”‚
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ extract.py
â”‚ â”œâ”€â”€ transform.py
â”‚ â””â”€â”€ load.py
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env (not included in git)

yaml
Copy code

---

## âš™ï¸ Environment Variables

Create a `.env` file in the project root and add:

Airflow User
AIRFLOW_WWW_USER_USERNAME=
AIRFLOW_WWW_USER_PASSWORD=

PostgreSQL Database
DB_HOST=
DB_USER=
DB_PASSWORD=
DB_NAME=
DB_PORT=

AWS S3 Credentials
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=

Airflow Security
AIRFLOW__WEBSERVER__SECRET_KEY=

yaml
Copy code

> âš ï¸ **Important:** Never upload `.env` to GitHub.  

---

## ğŸ³ Running the Project with Docker

Ensure **Docker** and **Docker Compose** are installed.

### 1ï¸âƒ£ Build Airflow Environment
```bash
docker compose build
2ï¸âƒ£ Initialize Airflow Database
bash
Copy code
docker compose up airflow-init
3ï¸âƒ£ Start all Airflow Services
bash
Copy code
docker compose up -d
4ï¸âƒ£ Access Airflow UI
Visit:
ğŸ‘‰ http://localhost:8080

Login using your .env Airflow credentials.

ğŸ“Œ ETL Pipeline Flow
scss
Copy code
Extract (S3) â†’ Transform (Clean/Validate) â†’ Load (PostgreSQL)
1. extract.py
Downloads required files from AWS S3

Saves them into a temporary folder

2. transform.py
Cleans, validates, and restructures the data

Performs conversions, handling nulls, formatting

3. load.py
Inserts transformed data into PostgreSQL

Creates table automatically if missing

ğŸ§ª Testing the ETL Pipeline
Open Airflow UI

Search for the ETL DAG

Enable the DAG

Click Trigger DAG

Track logs and execution from the Airflow UI.

ğŸ“¦ Technology Stack
Apache Airflow

Docker + Docker Compose

AWS S3

PostgreSQL

Python
